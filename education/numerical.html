<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="University of Houston" />
  <title>Introduction to Numerical Analysis</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Introduction to Numerical Analysis</h1>
<p class="author">University of Houston</p>
<p class="date">Spring 2022</p>
</header>
<h1 id="concepts">Concepts</h1>
<h2 class="unnumbered" id="stopping-criteria.">Stopping criteria.</h2>
<p>Absolute error Relative error</p>
<h2 class="unnumbered"
id="approximations-and-convergence.">Approximations and
convergence.</h2>
<p>Let <span class="math inline">\(Q\)</span> be a quantity and <span
class="math inline">\(A_h\)</span> an approximation of <span
class="math inline">\(Q\)</span>. Let <span class="math inline">\(h \neq
0\)</span>. If <span class="math inline">\(k&gt;0\)</span>, then <span
class="math inline">\(A_h\)</span> is an <span
class="math inline">\(O\left(h^k\right)\)</span> approximation of <span
class="math inline">\(Q\)</span> if and and only if there exists a
constant <span class="math inline">\(C &gt; 0\)</span> such that <span
class="math display">\[\left|Q - A_h\right| \leq C |h|^k\]</span></p>
<h2 class="unnumbered" id="taylors-theorem.">Taylor’s theorem.</h2>
<p>Taylor’s theorem with Lagrange form of remainder.</p>
<p>Let <span class="math inline">\(n \in \mathbb{N}\)</span> and <span
class="math inline">\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span> be
<span class="math inline">\(n + 1\)</span> times differentiable at <span
class="math inline">\(x = a\)</span>.</p>
<p><span class="math display">\[f(x) = f(a) + \sum_{k = 1}^{n}
\frac{1}{k!}f^{(k)}(x-a)^k + R(x)\]</span></p>
<p>Lagrange form of the remainder: for all <span class="math inline">\(x
\in [a,b]\)</span> there exists <span class="math inline">\(z_k \in
(a,x)\)</span> such that <span class="math display">\[R(x) =
\frac{1}{(k+1)!}f^{(k+1)}(z_k) (x-a)^{k+1}\]</span></p>
<h2 class="unnumbered" id="multivariable-chain-rule.">Multivariable
chain rule.</h2>
<p>Let <span class="math inline">\(z = f(x,y)\)</span>, where <span
class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span> are functions of <span
class="math inline">\(t\)</span>. Then, <span
class="math display">\[\frac{\mathrm{d}z}{\mathrm{d}t} = \frac{\partial
f}{\partial x}\frac{\mathrm{d}x}{\mathrm{d}t} + \frac{\partial
f}{\partial y}\frac{\mathrm{d}y}{\mathrm{d}t}\]</span></p>
<h2 class="unnumbered" id="multivariable-derivatives.">Multivariable
derivatives.</h2>
<p>Let <span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span>. <span class="math display">\[f&#39;(\mathbf{x}) =
\left[ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2},
\cdots, \frac{\partial f}{\partial x_n} \right]\]</span></p>
<p>Gradient. <span
class="math display">\[\operatorname{grad}{f(\mathbf{x})} = \nabla
f(\mathbf{x}) = (f&#39;(\mathbf{x}))^{\top}\]</span> The gradient points
in the greatest direction of increase.</p>
<p><span class="math display">\[f&#39;&#39;(\mathbf{x}) = (\nabla
f)&#39;(\mathbf{x})\]</span> Due to equality of mixed partials, <span
class="math inline">\(f&#39;&#39;(\mathbf{x})\)</span> is a symmetric
matrix.</p>
<p>Let <span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}^m\)</span>. <span class="math display">\[f&#39;(\mathbf{x}) =
\begin{bmatrix}
   \dfrac{\partial f_1}{\partial x_1} &amp; \dfrac{\partial
f_1}{\partial x_2} &amp; \cdots &amp; \dfrac{\partial f_1}{\partial x_n}
\\
   \dfrac{\partial f_2}{\partial x_1} &amp; \dfrac{\partial
f_2}{\partial x_2} &amp; \cdots &amp; \dfrac{\partial f_2}{\partial x_n}
\\
   \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
   \dfrac{\partial f_m}{\partial x_1} &amp; \dfrac{\partial
f_m}{\partial x_2} &amp; \cdots &amp; \dfrac{\partial f_m}{\partial x_n}
\\
\end{bmatrix}\]</span></p>
<h2 class="unnumbered" id="critical-points.">Critical points.</h2>
<p>Let <span class="math inline">\(f:\mathbb{R}^n \rightarrow
\mathbb{R}\)</span>. A critical point of <span
class="math inline">\(f\)</span> is a point <span
class="math inline">\(\mathbf{p} \in \mathbb{R}^n\)</span> such that
<span class="math inline">\(\nabla f(\mathbf{p}) =
\mathbf{0}\)</span>.</p>
<h2 class="unnumbered" id="symmetric-matrices.">Symmetric matrices.</h2>
<p>Let <span class="math inline">\(A\)</span> be a symmetric matrix.
<span class="math inline">\(A\)</span> is:</p>
<ul>
<li><p>positive definite if <span class="math inline">\(\det ( A_k) &gt;
0\)</span>.</p></li>
<li><p>negative definite if <span class="math inline">\((-1)^k \det (
A_k) &gt; 0\)</span>.</p></li>
<li><p>neither if it is neither positive or negative definite.</p></li>
</ul>
<h2 class="unnumbered" id="local-extrema.">Local extrema.</h2>
<p>Let <span class="math inline">\(f:\mathbb{R}^n \rightarrow
\mathbb{R}\)</span> and <span class="math inline">\(\mathbf{p}\)</span>
a critical point. If <span
class="math inline">\(f&#39;&#39;(\mathbf{p})\)</span> is:</p>
<ul>
<li><p>positive definite, then it is a local minimum.</p></li>
<li><p>negative definite, then it is a local maximum.</p></li>
<li><p>neither, then it is a saddle point.</p></li>
</ul>
<h1 id="algorithms">Algorithms</h1>
<h2 class="unnumbered" id="bisection-method.">Bisection method.</h2>
<p>Let <span class="math inline">\(f:[a,b] \rightarrow
\mathbb{R}\)</span> be a continuous function and <span
class="math inline">\([a,b]\)</span> brackets a zero of <span
class="math inline">\(f(x)\)</span>.</p>
<pre><code>Let $p_{n+1} = \dfrac{a_n + b_n}{2}$
If $f(p_n) = 0$, stop.
If $f(a_n)f(p_n) &lt; 0$, then
    $a_{n+1} = a_n$
    $b_{n+1} = p_n$
Else,
    $a_{n+1} = p_n$
    $b_{n+1} = b_n$</code></pre>
<p>Conclusion: <span class="math inline">\(p_{n+1}\)</span> approximates
within <span class="math inline">\(\dfrac{b-a}{2^{n+1}}\)</span>
Convergence: <span class="math inline">\(O(h)\)</span></p>
<h3 class="unnumbered" id="lazy-bisection-method.">Lazy bisection
method.</h3>
<p>Same as bisection method, but does not check if <span
class="math inline">\(f\left(p_n\right) = 0\)</span>, and modify the
check to <span class="math inline">\(f(a_n)f(p_n) \leq 0\)</span></p>
<h2 class="unnumbered" id="newtons-method.">Newton’s method.</h2>
<p>Let <span class="math inline">\(f:[a,b] \rightarrow
\mathbb{R}\)</span> be a twice differentiable function and there exists
<span class="math inline">\(p\)</span> such that <span
class="math inline">\(f(p) = 0\)</span> and <span
class="math inline">\(f&#39;(p) \neq 0\)</span>. Let <span
class="math inline">\(p_0\)</span> be an initial guess. <span
class="math display">\[p_{n+1} = p_n
-  \frac{f\left(p_n\right)}{f&#39;\left(p_n\right)}\]</span></p>
<p>Convergence: <span class="math inline">\(O(h^2)\)</span>. It can fail
if <span class="math inline">\(p_0\)</span> is not chosen close
enough.</p>
<h2 class="unnumbered" id="secant-method.">Secant method.</h2>
<p>Same as Newton’s method, except <span
class="math inline">\(f&#39;\left(p_n\right)\)</span> is replaced by a
secant line approximation. Requires two initial guesses, <span
class="math inline">\(p_0\)</span> and <span
class="math inline">\(p_1\)</span>. <span
class="math display">\[f&#39;\left(p_n\right) \approx
\frac{f\left(p_n\right) - f\left(p_{n-1}\right)}{p_n - p_{n-1}}\]</span>
<span class="math display">\[p_{n+1} = p_n -
\frac{f\left(p_n\right)\left(p_n - p_{n-1}\right)}{f\left(p_n\right) -
f\left(p_{n-1}\right)}\]</span></p>
<p>Convergence: <span class="math inline">\(O(h^r)\)</span>, where <span
class="math inline">\(r=\dfrac{1 + \sqrt{5}}{2}\)</span>.</p>
<h2 class="unnumbered" id="false-position-method">False position
method</h2>
<p>Same as bisection method, but replace midpoint with <span
class="math inline">\(x\)</span>-intercept of the secant line.</p>
<p><span class="math display">\[p_{n+1} = a_1 -
\frac{f\left(a_1\right)\left(b_1 - a_1\right)}{f\left(b_1\right) -
f\left(a_1\right)}\]</span></p>
<p>Convergence: More rapid than the bisection method.</p>
<h2 class="unnumbered" id="fixed-point-iteration.">Fixed point
iteration.</h2>
<p>Similar to Newton’s method. Choose <span
class="math inline">\(p_0\)</span> such that <span
class="math inline">\(-\dfrac{2}{f&#39;(x)} &lt;
-\dfrac{1}{f&#39;\left(p_0\right)} &lt; 0\)</span> for <span
class="math inline">\(x \in [a,b]\)</span></p>
<p><span class="math display">\[p_{n+1} = p_n -
\frac{f\left(p_n\right)}{f&#39;\left(p_0\right)}\]</span></p>
<h2 class="unnumbered" id="modified-newtons-method.">Modified Newton’s
method.</h2>
<p>Let <span class="math inline">\(f\)</span> be sufficiently
smooth.</p>
<p><span class="math display">\[p_{n+1} = p_n -
\frac{f\left(p_n\right)f&#39;\left(p_n\right)}{\left[f&#39;\left(p_n\right)\right]^2
- f\left(p_n\right)f&#39;&#39;\left(p_n\right)}\]</span></p>
<h2 class="unnumbered"
id="forward-and-backward-difference-approximations.">Forward and
backward difference approximations.</h2>
<p>Let <span class="math inline">\(f\)</span> be twice
differentiable.</p>
<p><span class="math display">\[f&#39;(x) \approx
\frac{f(x+h)-f(x)}{h}\]</span></p>
<p>Error:</p>
<p><span class="math display">\[\left| f&#39;(x) -
f_{\text{approx}}&#39;(x) \right| = \frac{M}{2} \left|h\right|\]</span>
<span class="math display">\[M = \max_{a\leq x \leq b}
\left|f&#39;&#39;(x)\right|\]</span></p>
<p><span class="math inline">\(O(h)\)</span> approximation.</p>
<h2 class="unnumbered" id="centered-difference-approximation.">Centered
difference approximation.</h2>
<p>Let <span class="math inline">\(f\)</span> be three times
differentiable.</p>
<p><span class="math display">\[f&#39;(x) \approx
\frac{f(x+h)-f(x-h)}{2h}\]</span></p>
<p>Error: <span class="math display">\[\left| f&#39;(x) -
f_{\text{approx}}&#39;(x) \right| = \frac{M}{6}h^2\]</span> <span
class="math display">\[M = \max_{a\leq x \leq b}
\left|f&#39;&#39;&#39;(x)\right|\]</span></p>
<p><span class="math inline">\(O(h^2)\)</span> approximation.</p>
<h2 class="unnumbered"
id="centered-difference-approximation-of-second-derivative.">Centered
difference approximation of second derivative.</h2>
<p>The centered difference can also be applied to find the second
derivative.</p>
<p><span class="math display">\[f&#39;&#39;(x) \approx
\frac{f(x+h)+f(x-h)-2f(x)}{h^2}\]</span></p>
<p>Error: <span class="math display">\[\left| f&#39;&#39;(x) -
f_{\text{approx}}&#39;&#39;(x) \right| = \frac{M}{12}h^2\]</span> <span
class="math display">\[M = \max_{a\leq x \leq b}
\left|f^{(4)}(x)\right|\]</span></p>
<p><span class="math inline">\(O(h^2)\)</span> approximation.</p>
<h2 class="unnumbered" id="point-forward-and-backward.">3 point forward
and backward.</h2>
<p><span class="math display">\[f&#39;(x) \approx
\frac{1}{2h}\left[-3f(x)+4f(x+h)-f(x+2h) \right]\]</span></p>
<p><span class="math display">\[\left| f&#39;(x) -
f_{\text{approx}}&#39;(x) \right| = \frac{M}{3}h^2\]</span> <span
class="math display">\[M = \max_{a\leq x \leq b}
\left|f&#39;&#39;&#39;(x)\right|\]</span></p>
<p><span class="math inline">\(O(h^2)\)</span> approximation. But
centered difference is better, has half the error.</p>
<h2 class="unnumbered" id="richardson-extrapolation">Richardson
extrapolation</h2>
<p>Suppose Q is a quantity and <span
class="math inline">\(N_1(h)\)</span> is an approximation for <span
class="math inline">\(Q\)</span> of the form <span
class="math display">\[Q = N_1(h) + K_1h + E(h)h^2\]</span></p>
<p>Single-step Richardson’s extrapolation: creates an <span
class="math inline">\(O(h^2)\)</span> approximation. <span
class="math display">\[N_2(h) = 2N_1\left(\frac{h}{2}\right) -
N_1(h)\]</span></p>
<p>Multiple-step Richardson’s extrapolation: creates an <span
class="math inline">\(O(h^k)\)</span> approximation.</p>
<p><span class="math display">\[N_k (h) = N_{k-1}
\left(\frac{h}{2}\right) + \frac{1}{2^{k-1}-1} \left[ N_{k-1}
\left(\frac{h}{2}\right) - N_{k-1} (h) \right]\]</span></p>
<p>Convergence: <span class="math inline">\(O(h^k)\)</span>.</p>
<h2 class="unnumbered" id="richardson-extrapolation-1">Richardson
extrapolation</h2>
<p>Suppose Q is a quantity and <span
class="math inline">\(N_1(h)\)</span> is an approximation for <span
class="math inline">\(Q\)</span> of the form <span
class="math display">\[Q = N_1(h) + K_1h^2 + E(h)h^4\]</span></p>
<p>Single-step Richardson’s extrapolation: creates an <span
class="math inline">\(O(h^4)\)</span> approximation. <span
class="math display">\[N_2(h) = \frac{4}{3}N_1\left(\frac{h}{2}\right) -
\frac{1}{3}N_1(h)\]</span></p>
<p>Multiple-step Richardson’s extrapolation: creates an <span
class="math inline">\(O(h^{2k})\)</span> approximation.</p>
<p><span class="math display">\[N_k (h) = N_{k-1}
\left(\frac{h}{2}\right) + \frac{1}{4^{k-1}-1} \left[ N_{k-1}
\left(\frac{h}{2}\right) - N_{k-1} (h) \right]\]</span></p>
<p>Convergence: <span class="math inline">\(O(h^{2k})\)</span></p>
<h2 class="unnumbered" id="midpoint-method-integration.">Midpoint method
(integration).</h2>
<p>Let <span class="math inline">\(f: [a,b] \rightarrow
\mathbb{R}\)</span> be twice differentiable. Let <span
class="math inline">\(n \in \mathbb{N}\)</span>. Then <span
class="math inline">\(h = \dfrac{b-a}{n}\)</span>, <span
class="math inline">\(x_i = a + ih\)</span> and <span
class="math inline">\(c_i = \dfrac{x_{i-1} + x_i}{2}\)</span>.</p>
<p><span class="math display">\[\operatorname{Mid}_n (f,a,b) = h
\sum_{i=1}^{n} f(c_i)\]</span></p>
<p>Error: <span class="math display">\[\left| \int_a^b f(x) \ \mathrm{d}
x  - \operatorname{Mid}_n (f,a,b) \right| \leq \frac{M(b-a)}{24}
h^2\]</span> <span class="math display">\[M = \max_{a\leq x \leq b}
\left|f&#39;&#39;(x)\right|\]</span></p>
<p>Convergence: <span class="math inline">\(O(h^2)\)</span></p>
<h2 class="unnumbered" id="trapezoid-method.">Trapezoid method.</h2>
<p><span class="math display">\[\begin{aligned}
    \operatorname{Trap}_n (f,a,b) &amp;= \frac{h}{2} \sum_{i=1}^{n}
\left[f(x_i-1) + f(x_i) \right] \\
    &amp;= \frac{1}{2}\left[ f(a) + f(b) \right]h + h
\sum_{i=1}^{n-1}f(x_i)\end{aligned}\]</span></p>
<p>Convergence: <span class="math inline">\(O(h^2)\)</span>. Worse than
midpoint method.</p>
<h2 class="unnumbered" id="simpsons-method.">Simpson’s method.</h2>
<p><span class="math display">\[\operatorname{Simp}_n (f,a,b) =
\frac{1}{3}\operatorname{Trap}_n (f,a,b) +
\frac{2}{3}\operatorname{Mid}_n (f,a,b)\]</span></p>
<p>Convergence: <span class="math inline">\(O(h^4)\)</span>.</p>
<h2 class="unnumbered" id="romberg-method.">Romberg method.</h2>
<p>Richardson extrapolation applied to trapezoid method. <span
class="math display">\[N_1 (h) = \operatorname{Trap}_1 (f,a,b)\]</span>
<span class="math display">\[N_k (h) = N_{k-1} \left(\frac{h}{2}\right)
+ \frac{1}{4^{k-1}-1} \left[ N_{k-1} \left(\frac{h}{2}\right) - N_{k-1}
(h) \right]\]</span></p>
<p>Convergence: <span class="math inline">\(O(h^{2k})\)</span></p>
<h2 class="unnumbered" id="eulers-method.">Euler’s method.</h2>
<p><span class="math display">\[y_{n+1} = y_n + hf(t_n, y_n)\]</span>
Convergence: <span class="math inline">\(O(h^2)\)</span> across each
step, <span class="math inline">\(O(h)\)</span> across the whole
interval.</p>
<h2 class="unnumbered"
id="midpoint-method-initial-value-problem.">Midpoint method (initial
value problem).</h2>
<p><span class="math display">\[\begin{aligned}
    z_n &amp;= y_n + \frac{1}{2}hf(t_n, y_n) \\
    y_{n+1} &amp;= y_n + hf\left(t_n + \frac{h}{2},
z_n\right)\end{aligned}\]</span> Convergence: <span
class="math inline">\(O(h^3)\)</span> across each step, <span
class="math inline">\(O(h^2)\)</span> across the whole interval.</p>
<h2 class="unnumbered" id="modified-eulers-method.">Modified Euler’s
method.</h2>
<p><span class="math display">\[\begin{aligned}
    z_n &amp;= y_n + hf(t_n, y_n) \\
    y_{n+1} &amp;= y_n + \frac{1}{2}h\left[f(t_n, y_n) + f(t_{n+1}, z_n)
\right]\end{aligned}\]</span> Convergence: <span
class="math inline">\(O(h^3)\)</span> across each step, <span
class="math inline">\(O(h^2)\)</span> across the whole interval.</p>
<h2 class="unnumbered" id="taylor-methods.">Taylor methods.</h2>
<p>Given an initial value problem, <span
class="math display">\[\begin{aligned}
    y&#39;(t) = f(t,y)\end{aligned}\]</span></p>
<p>Start with Taylor’s theorem: <span class="math display">\[y_{k+1} =
y_k + y&#39;_k h + \frac{1}{2}y&#39;&#39;_k h^2 + \cdots\]</span>
Differentiate the initial value problem as many times as needed. Use the
multivariable chain rule! Substitute derivatives into Taylor’s
theorem.</p>
<h2 class="unnumbered" id="runge-kutta">Runge-Kutta</h2>
<p>Let <span class="math display">\[\begin{aligned}
    k_1 &amp;= y_n + f(t_n, y_n)\\
    k_2 &amp;= y_n + f\left(t_n + \frac{h}{2},y_n +
\frac{k_1}{2}\right)\\
    k_3 &amp;= y_n + f\left(t_n + \frac{h}{2}, y_n +
\frac{k_2}{2}\right)\\
    k_4 &amp;= y_n + f(t_n + h, y_n + k_3)\end{aligned}\]</span></p>
<p><span class="math display">\[y_{n+1} = y_n + \frac{1}{6}(k_1 + 2k_2 +
2k_3 + k_4)\]</span></p>
<h2 class="unnumbered" id="shooting-method.">Shooting method.</h2>
<p>Consider a boundary condition problem. Transform the problem into an
initial value problem with variable values. Create a function of those
variable values and output the right endpoint value. Check which right
endpoints match the the original boundary value problems. Use the secant
method to find the values. This works for certain nonlinear boundary
value problems as well.</p>
<h2 class="unnumbered"
id="linear-second-order-two-point-boundary-value-problems.">Linear
second order two-point boundary value problems.</h2>
<p><span class="math display">\[y&#39;&#39; = py&#39;+qy+h, \quad x \in
[a,b]\]</span> </p>
<p>Initial conditions are given by <span class="math display">\[A
\begin{bmatrix}y(a) \\ y(b) \\y&#39;(a) \\ y&#39;(b) \\ \end{bmatrix} =
\begin{bmatrix}\alpha \\ \beta\end{bmatrix}\]</span></p>
<p>In other words, <span class="math display">\[\begin{aligned}
    y(a) + y&#39;(a) &amp;= \alpha \\
    y(b) + y&#39;(b) &amp;= \beta\end{aligned}\]</span></p>
<p>Start by solving the three fundamental initial value problems. <span
class="math display">\[\begin{aligned}
    u_1&#39;&#39; &amp;= pu_1&#39; + qu_1 \\
    u_1 (a) &amp;= 1\\
    u_1&#39; (a) &amp;= 0\\\end{aligned}\]</span> <span
class="math display">\[\begin{aligned}
    u_2&#39;&#39; &amp;= pu_2&#39; + qu_2 \\
    u_2 (a) &amp;= 0\\
    u_2&#39; (a) &amp;= 1\\\end{aligned}\]</span> <span
class="math display">\[\begin{aligned}
    w&#39;&#39; &amp;= pw&#39; + qw + h \\
    w (a) &amp;= 0\\
    w&#39; (a) &amp;= 0\\\end{aligned}\]</span></p>
<p>If there is a solution, it is <span class="math inline">\(C_1,
C_2\)</span> such that</p>
<p><span class="math display">\[y(x) = C_1 u_1 + C_2 u_2 +
w\]</span></p>
<p>The problem has a unique solution if and only if <span
class="math inline">\(M\)</span> is invertible (<span
class="math inline">\(\det M \neq 0\)</span>). <span
class="math display">\[M = A \begin{bmatrix}
u_1 (a) &amp; u_2 (a) \\
u_1 (b) &amp; u_2 (b) \\
u_1&#39; (a) &amp; u_2&#39; (a) \\
u_1&#39; (b) &amp; u_2&#39; (b)
\end{bmatrix}\]</span></p>
<p>Then, let <span class="math display">\[\mathbf{v} =
\begin{bmatrix}\alpha \\ \beta\end{bmatrix} - A \begin{bmatrix}w(a) \\
w(b) \\w&#39;(a) \\ w&#39;(b) \\ \end{bmatrix}\]</span></p>
<p>Finally, <span class="math inline">\(C_1\)</span> and <span
class="math inline">\(C_2\)</span> are determined by</p>
<p><span class="math display">\[\begin{bmatrix}C_1 \\ C_2\end{bmatrix} =
M^{-1} \mathbf{v}\]</span></p>
<h2 class="unnumbered" id="gradient-descent.">Gradient descent.</h2>
<p>Gradient descent. Let <span class="math inline">\(f: \mathbb{R}^n
\rightarrow \mathbb{R}\)</span> be continuously differentiable and <span
class="math inline">\(p \in \mathbb{R}^n\)</span> be a local minimum of
<span class="math inline">\(f\)</span>. Let <span
class="math inline">\(p_0\)</span> be an initial guess.</p>
<pre><code>for $k = 0, 1, \dots$
    set $M = \displaystyle \sum_{i=1}^{n} \left| \frac{\partial f}{\partial x_i} \right| + 1 $
    let $s = 1$
    create $\text{temp} = p_0 - s \cdot \displaystyle \frac{1}{M} \nabla f(p_k)$
    while $f(\text{temp}) \geq f(p_k)$
        $s = \displaystyle \frac{s}{2}$
        $\text{temp} = p_0 - s \cdot \displaystyle \frac{1}{M} \nabla f(p_k) $
    $p_{k+1} = \text{temp}$
next $k$</code></pre>
<p>Convergence is only guaranteed when <span
class="math inline">\(f&#39;&#39;(x)\)</span> is positive definite.
Convergence is also slow.</p>
<h2 class="unnumbered" id="newtons-method-multivariable.">Newton’s
method (multivariable).</h2>
<p>Let <span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}^n\)</span> be a twice continuously differentiable function
and <span class="math inline">\(\mathbf{u} \in \mathbb{R}^n\)</span>
such that <span class="math inline">\(f(\mathbf{u}) =
\mathbf{0}\)</span>. Let <span
class="math inline">\(\mathbf{x}_0\)</span> be an initial guess.</p>
<p><span class="math display">\[\mathbf{x}_{n+1} = \mathbf{x}_{n} -
\left(f&#39;(\mathbf{x}_{n})\right)^{-1} f(\mathbf{x}_{n})\]</span>
Convergence: <span class="math inline">\(O(h^2)\)</span>.</p>
<h2 class="unnumbered" id="finite-differences.">Finite differences.</h2>
<p>Use to approximate solutions of second order linear problems with
Dirichlet boundary conditions. <span
class="math display">\[\begin{aligned}
    u&#39;&#39;(x) &amp;= P(x)u&#39;(x) + Q(x)u(x) + f(x) \\
    u(a) &amp;= \alpha, \quad u(b) = \beta\end{aligned}\]</span> Let
<span class="math inline">\(h = \dfrac{b-a}{n}\)</span> and <span
class="math inline">\(x_i = a + ih\)</span> for <span
class="math inline">\(i\in \{0,1,\dots,n-1 \}\)</span>. Let <span
class="math display">\[\begin{aligned}
    u_i &amp;= u(x_i) \\
    P_i &amp;= P(x_i) \\
    Q_i &amp;= Q(x_i) \\
    f_i &amp;= f(x_i)\end{aligned}\]</span> Approximate <span
class="math inline">\(u&#39;(x)\)</span> and <span
class="math inline">\(u&#39;&#39;(x)\)</span> by the centered difference
approximations. <span class="math display">\[\begin{aligned}
    u&#39;(x_i) &amp;\approx \dfrac{u(x_i + h) - u(x_i - h)}{2h} \approx
\dfrac{u_{i+1} - u_{i-1}}{2h}\\
    u&#39;&#39;(x_i) &amp;\approx \dfrac{u(x_i + h) + u(x_i - h) -
2u(x_i)}{h^2} \approx \dfrac{u_{i+1} + u_{i-1} -
2u_i}{h^2}\end{aligned}\]</span> Finally, solve for weights of <span
class="math inline">\(u_i\)</span> <span
class="math display">\[\begin{aligned}
    \frac{u_{i+1} + u_{i-1} - 2u_i}{h^2} &amp;= P_i \frac{u_{i+1} -
u_{i-1}}{2h} + Q_i u_i + f_i \\
    u_{i+1} + u_{i-1} - 2u_i &amp;= \frac{h}{2} P_i (u_{i+1} - u_{i-1})
+ h^2 Q_i u_i + h^2 f_i \\
    \left(1 + \frac{h}{2} P_i \right) u_{i-1} + \left(-2 - h^2
Q_i\right) u_i + \left(1 - \frac{h}{2} P_i \right) u_{i+1} &amp;= h^2
f_i\end{aligned}\]</span> For every <span class="math inline">\(i \in
\{1,2,\dots,n-1\}\)</span>, substitute into the equation to create the
linear system. <span class="math inline">\(M =\)</span> <span
class="math display">\[\begin{bmatrix}
    -2 - h^2 Q_1 &amp; 1 - \dfrac{h}{2} P_1 &amp; 0 &amp; 0 &amp; 0
&amp; \cdots &amp; 0\\
    1 + \dfrac{h}{2} P_2 &amp; -2 - h^2 Q_2 &amp; 1 - \dfrac{h}{2} P_2
&amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
    0 &amp; 1 + \dfrac{h}{2} P_3 &amp; -2 - h^2 Q_3 &amp; 1 -
\dfrac{h}{2} P_3 &amp; 0&amp;  \cdots &amp; 0 \\
    \vdots &amp; \vdots &amp; \ddots &amp; \ddots  &amp; \ddots &amp;
\ddots &amp; \vdots \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 + \dfrac{h}{2} P_{n-2} &amp; -2 -
h^2 Q_{n-2} &amp; 1 - \dfrac{h}{2} P_{n-2} \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;  1 + \dfrac{h}{2} P_{n-1}
&amp; -2 - h^2 Q_{n-1}
\end{bmatrix}\]</span> <span class="math display">\[\mathbf{f} =
\begin{bmatrix}
    h^2 f_1 - \alpha\left(1 + \dfrac{h}{2} P_1\right) \\
    h^2 f_2 \\
    h^2 f_3 \\
    \vdots \\
    h^2 f_{n-1} - \beta \left(1 - \dfrac{h}{2} P_{n-1}\right)
\end{bmatrix}\]</span> Finally, solve <span
class="math inline">\(M\mathbf{u} = \mathbf{f}\)</span>. The matrix
<span class="math inline">\(M\)</span> is tri-diagonal, which allows the
use of the Thomas algorithm to solve with less steps than inverting the
matrix.</p>
<h2 class="unnumbered" id="thomas-algorithm">Thomas algorithm</h2>
<p>Consider a tridiagonal system <span class="math inline">\(a_ix_{i+1}
+ b_i x_i + c_i x_{i-1} = d_i\)</span>.</p>
<p><span class="math display">\[\begin{bmatrix}
   b_1 &amp; c_1 &amp;        &amp;        &amp;  0      \\
   a_2 &amp; b_2 &amp; c_2    &amp;        &amp;         \\
       &amp; a_3 &amp; b_3    &amp; \ddots &amp;         \\
       &amp;     &amp; \ddots &amp; \ddots &amp; c_{n-1} \\
   0   &amp;     &amp;        &amp; a_n    &amp; b_n
\end{bmatrix}
\begin{bmatrix}
   x_1    \\
   x_2    \\
   x_3    \\
   \vdots \\
   x_n
\end{bmatrix}
=
\begin{bmatrix}
   d_1    \\
   d_2    \\
   d_3    \\
   \vdots \\
   d_n
\end{bmatrix}\]</span></p>
<pre><code>Let $\mathbf{u} = \mathbf{b}$
For $i = 2, 3, \dots, n$
    let $m = \dfrac{a_{i-1}}{b_{i-1}}$
    let $b_i = b_i - m\cdot c_{i-1}$
    let $f_i = f_i - m \cdot f_{i-1}$
Let $u_n = \dfrac{f_n}{b_n}$
Perform backwards substitution. For $i = n-1, n-2, \dots, 1$
    Let $u_i = \dfrac{f_i - c_i \cdot u_{i+1}}{b_i}$</code></pre>
</body>
</html>
